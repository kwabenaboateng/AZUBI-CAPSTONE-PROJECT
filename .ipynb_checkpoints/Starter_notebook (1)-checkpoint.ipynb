{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoZGjB_9Uiij"
   },
   "source": [
    "# IMPORTANT \n",
    "\n",
    "## Install latest version of packages to be used in the code\n",
    "\n",
    "The latest version of libraries need to be installed as per competition rules and kindly adhere to that and install the updated version of libraries in the code. \n",
    "\n",
    "## Please set random seed so that reproducible answers are attained\n",
    "\n",
    "Wherever randomness is expected, do select the random seed so that the results are reproducible. Reproducibility of results is a **very important** component of model development without which reliable models are not attained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Wjp74XdUQta",
    "outputId": "1b79eaab-cceb-4666-e85b-452a4a7010f9"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn numpy pandas catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYI4nuYZUw3C"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score,accuracy_score,classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('always') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbf9wqJGU20y"
   },
   "source": [
    "## Loading test and train datasets \n",
    "\n",
    "We will load the train and test datasets and do some basic level of EDA to understand the pattern of features in the data \n",
    "\n",
    "* <b> Train data: </b> This is the data which we will be using to train the model. Since we are solving a classification problem, we will have a column in train dataset corresponding to the target labels. \n",
    "* <b> Test data: </b> This is the data on which the predictions will be made based on the model trained on train dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "Es4uUithU2QZ",
    "outputId": "79047a21-ef24-46b7-dba3-c6bbf3a4954d"
   },
   "outputs": [],
   "source": [
    "################# Reading train and test datasets\n",
    "train_data         = pd.read_csv('Train.csv')\n",
    "test_data          = pd.read_csv('Test.csv')\n",
    "\n",
    "\n",
    "\n",
    "target_column_name = ['income_above_limit']\n",
    "\n",
    "########## The target column to be used for training \n",
    "target_column      = train_data[target_column_name]\n",
    "\n",
    "########## Since ID is a unique identifier, it must be dropped \n",
    "Cols2drop          = ['ID']\n",
    "\n",
    "\n",
    "######### Feature set corresponding to train and test data\n",
    "train_df           = train_data.drop(Cols2drop+target_column_name,axis=1)\n",
    "test_df            = test_data.drop(Cols2drop,axis=1)\n",
    "\n",
    "print(colored(f'The shape of train data is    {train_df.shape}     ','green',attrs=['bold']))\n",
    "\n",
    "print(colored(f'The shape of target column is {target_column.shape}','green',attrs=['bold']))\n",
    "\n",
    "print(colored(f'The shape of test data is     {test_df.shape}      ','blue',attrs=['bold']))\n",
    "\n",
    "print('------------------------------------------------------------------------------')\n",
    "print(colored('The train data looks like below :- \\n','green'))\n",
    "display(train_df.head(5))\n",
    "\n",
    "print('------------------------------------------------------------------------------')\n",
    "print(colored('The test data looks like below :- \\n','blue'))\n",
    "display(test_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3eHwsyh73-0",
    "outputId": "4144b00e-f95d-48ec-f50e-f52cd1282a3e"
   },
   "outputs": [],
   "source": [
    "########### Encoding the target column \n",
    "\n",
    "target_column['income_above_limit'] = target_column['income_above_limit'].map({'Above limit':1,'Below limit':0})\n",
    "target_column['income_above_limit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssWWATyLYQEr"
   },
   "source": [
    "<b>Class imbalance </b> <br>\n",
    "\n",
    "\n",
    "We will be seeing the class imbalance using value_counts() method of pandas dataframe and use histogram to plot the imbalances\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "ORYzhnJUXd2p",
    "outputId": "4539bb67-ad4b-4cb6-abb6-58af2c53df60"
   },
   "outputs": [],
   "source": [
    "print('The class Imbalance in the data is given below')\n",
    "display(train_data['income_above_limit'].value_counts())\n",
    "print('---------------------------------------------------------------\\n')\n",
    "print('The class imbalance in terms of percentage is given below ')\n",
    "display(train_data['income_above_limit'].value_counts(normalize=True))\n",
    "print('----------------------------------------------------------------\\n')\n",
    "pct_df = pd.DataFrame(train_data['income_above_limit'].value_counts(normalize=True)).reset_index().rename({'index':'Target_values','income_above_limit':'Percentage'},axis=1)\n",
    "fig = px.bar(pct_df,x='Target_values',y='Percentage', height=400,width = 400,title='class imbalance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdXCqdVpZO5h"
   },
   "source": [
    "Clearly we have a highly imbalanced dataset available with us and hence we need to perform steps to mitigate the imbalance accordingly. The following methods could be used:- \n",
    "1. Downsample the majority class (Here majority class is 'Below limit') \n",
    "2. Upsample the minority class (Here, minority class is 'Above limit') \n",
    "3. Use class weights while performing model development <br>\n",
    "Reference : https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExK0qe-_bjWg"
   },
   "source": [
    "<b> NaN value analysis </b> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1HVJeYKXd5R"
   },
   "outputs": [],
   "source": [
    "def nan_value_plot(df):\n",
    "    nan_dict  = {}\n",
    "    for cols in df.columns:\n",
    "        nan_dict[cols] = df[cols].isna().sum()/df.shape[0]\n",
    "    nan_pct_df = pd.DataFrame.from_dict(nan_dict,orient='index').reset_index().rename({'index':'Columns',0:'NaN_pct'},axis=1)\n",
    "    fig = px.bar(nan_pct_df,x='Columns',y='NaN_pct', height=400,width = 400,title='NaN value percentage in each column')\n",
    "    fig.update_layout(\n",
    "                        xaxis = dict(\n",
    "                        tickfont = dict(size=5)))\n",
    "    fig.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "id": "xqpPz5N_Xd8C",
    "outputId": "a68b45a5-0294-451a-97ed-a1e841d7628c"
   },
   "outputs": [],
   "source": [
    "print(colored('We see the distribution of NaN values in train data as below','green',attrs=['bold']))\n",
    "nan_value_plot(train_df)\n",
    "\n",
    "print('-------------------------------------------------------------------------------------------------')\n",
    "print('\\n')\n",
    "print(colored('We see the distribution of NaN values in test data as below','blue',attrs=['bold']))\n",
    "nan_value_plot(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfUoDAEedW8b"
   },
   "source": [
    "<b> Comments:- </b>\n",
    "* There are columns with extremely high proportion of NaN values, we may drop them. \n",
    "* There are columns with NaN values that can be handled easily using imputations with mean, median (in case of numerical) or mode(in case of categorical) \n",
    "* Use Models like LightGBM, CatBoost or XGBoost that handles the NaN values implicitly while model training. \n",
    "* Observe that the proportion of NaN value distribution is same in train and test and select NaN value handling techniques accordingly. \n",
    "* Be creative üß† (but also be logical üòâ) !!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjzVdsKQexpj"
   },
   "source": [
    "I will personally drop all the columns where the proportion of NaN values is above 80% and proceed with columns/features that are left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdzU47_vXd_B"
   },
   "outputs": [],
   "source": [
    "nan_cols_drop  = []\n",
    "for cols in test_df.columns:\n",
    "    if test_df[cols].isna().sum()/test_df.shape[0] >0.8:\n",
    "        nan_cols_drop.append(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ekvv9En1XeCC",
    "outputId": "b52ed7c8-fc69-4527-a8d0-6c0817da9959"
   },
   "outputs": [],
   "source": [
    "print(colored(f'We will drop the following columns from both train and test data: ','yellow',attrs=['bold']))\n",
    "print(nan_cols_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ftntkr3MXeFh",
    "outputId": "27c3890f-a0ef-4811-a585-fa6c41a74885"
   },
   "outputs": [],
   "source": [
    "print('The shape of train and test data before dropping columns with high proportion of NaN values is - ')\n",
    "print(colored(f'The shape of train data is    {train_df.shape}     ','green',attrs=['bold']))\n",
    "\n",
    "print(colored(f'The shape of target column is {target_column.shape}','green',attrs=['bold']))\n",
    "\n",
    "print(colored(f'The shape of test data is     {test_df.shape}      ','blue',attrs=['bold']))\n",
    "\n",
    "train_df = train_df.drop(nan_cols_drop,axis=1)\n",
    "test_df  = test_df.drop(nan_cols_drop,axis=1)\n",
    "\n",
    "print('---------------------------------------------------------------------------------------------------')\n",
    "print('The shape of train and test data after dropping columns with high proportion of NaN values is - ')\n",
    "print(colored(f'The shape of train data is    {train_df.shape}     ','green',attrs=['bold']))\n",
    "\n",
    "print(colored(f'The shape of target column is {target_column.shape}','green',attrs=['bold']))\n",
    "\n",
    "print(colored(f'The shape of test data is     {test_df.shape}      ','blue',attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLZXfb8VgOKH"
   },
   "source": [
    "### Simple Baseline Validation strategy \n",
    "\n",
    "We will now do an 80-20 split of train data provided. As discussed previously, the participants are free to use the validation strategy of their own choice. \n",
    "\n",
    "Points to consider while selecting a validation strategy:\n",
    "* Make sure the model is not overfitting on train data. \n",
    "* CV score and leaderboard scores are in sync. \n",
    "* Stable validation strategy when using K Folds etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knl0lYHRXeQZ"
   },
   "outputs": [],
   "source": [
    "train, X_test, train_y, y_test = train_test_split(train_df, target_column, test_size=0.2, random_state=42,stratify=target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaWJ4WnPhlvT"
   },
   "source": [
    "### Model development ü§ñ üíª ü§ñ\n",
    "\n",
    "We will be straight away using a CatBoost model for training because it handles categorical features well, can implicitly handle NaN values, and can give a quick baseline (with minimal preprocessing) which can be used as a benchmark to be improved upon. \n",
    "\n",
    "<br>\n",
    "\n",
    "In the below steps, we will convert all the categorical columns to string datatype and capture the indices where string datatype is present which will then be used as an input for the CatBoost Classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-Q9dG7zjAvj"
   },
   "outputs": [],
   "source": [
    "cat_cols_index = np.where(train_df.dtypes=='object')[0]\n",
    "for i in range(len(train_df.columns)):\n",
    "    if i in cat_cols_index:\n",
    "        train[train_df.columns[i]]   = train[train_df.columns[i]].astype(str)\n",
    "        X_test[train_df.columns[i]]  = X_test[train_df.columns[i]].astype(str)\n",
    "        test_df[train_df.columns[i]] = test_df[train_df.columns[i]].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbDEel07XeWx",
    "outputId": "5ab58a61-0952-4a04-cd77-40cd7e353410"
   },
   "outputs": [],
   "source": [
    "model           = CatBoostClassifier(random_state=42,n_estimators =50 )\n",
    "_               = model.fit(train,train_y,cat_features= cat_cols_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eojVwf4_zxY0"
   },
   "source": [
    "Parameter tuning tips for CatBoost:\n",
    "\n",
    "üëì Do focus on parameters like n_estimators, max_depth, reg_lambda, reg_alpha, scale_pos_weight, learning_rate and explore other parameters from the link : https://catboost.ai/en/docs/references/training-parameters/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6uH4m5uk7kD",
    "outputId": "37665c64-3c53-474e-8a0b-d0f532037d7c"
   },
   "outputs": [],
   "source": [
    "acc_valid = accuracy_score(model.predict(X_test),y_test)\n",
    "\n",
    "print(colored(f'The accuracy attained on the validation set is {acc_valid}','green',attrs=['bold']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH9qJm_wnnnS"
   },
   "source": [
    "We got a good enough accuracy but is our model really performing that good ?? ü§î\n",
    "\n",
    "üëì Consider the class imbalance of the data given with respect to the metric assigned. We can get 94% accuracy just by classifying everything as 'Below limit' but that will mean that we must get an accuracy above 94% to ensure the models are learning properly. üëì \n",
    "\n",
    "üî≠ Let's investigate the classification report for both train and validation data and see how good the baseline is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jlXsgvgnZZp",
    "outputId": "923d6976-d4fe-42ae-d7b3-1e954c05d5aa"
   },
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print('The classification report only on the validation data is below-')\n",
    "print(colored(classification_report(y_test, model.predict(X_test)),'blue',attrs=['bold']))\n",
    "\n",
    "print('The classification report only on the train data is below-')\n",
    "print(colored(classification_report(train_y, model.predict(train)),'green',attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgzpbMFTn7ir"
   },
   "source": [
    "The performance of our minority class in terms of precision and recall is too low. Hence our F1 score is also very low. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WT5MfS46mDs"
   },
   "source": [
    "### A little hack \n",
    "\n",
    "Let's do a small hack though ü§ì ü§ì ü§ì\n",
    "\n",
    "We can use probability based thresholds and see how performance improves. We will select a lower threshold for class label 1.\n",
    "The default threshold is 0.5 which means that if the probability of 1 is above 0.5, then the predicted class is 1 else it is 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "We will lower the threshold to 0.4 and say that if the probability of class being 1 is above 0.4, then we can classify it as 1 and if it is less than 0.4, then it will be 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SnoBiaJL7D40",
    "outputId": "d009f5c7-f59d-4652-e9fe-1f289d671146"
   },
   "outputs": [],
   "source": [
    "thresh     = 0.4\n",
    "train_pred = np.where(model.predict_proba(train)[:,1]>thresh,1,0)\n",
    "test_pred  = np.where(model.predict_proba(X_test)[:,1]>thresh,1,0)\n",
    "\n",
    "print('\\n')\n",
    "print('The classification report only on the validation data is below-')\n",
    "print(colored(classification_report(y_test,test_pred),'blue',attrs=['bold']))\n",
    "\n",
    "print('The classification report only on the train data is below-')\n",
    "print(colored(classification_report(train_y, train_pred),'green',attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPPHhkd5-BJj"
   },
   "source": [
    "We do see some improvement in the performance because the f1 score on our validation data moved from 0.58 to 0.61. \n",
    "For more information about how the threshold is selected, please follow [ROC Curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) of sklearn and in general how ROC curve works üìö üìö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6yifLIyoEQb"
   },
   "source": [
    "### Submission Time üéâ\n",
    "\n",
    "We will now predict on the test data given and see what score we get on leaderboard. \n",
    "\n",
    "We will now download the file \"Sample_submission_1.csv\" and submit it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFlITBHen2HZ",
    "outputId": "0c86cee4-122c-45c0-9746-20683bb4d474"
   },
   "outputs": [],
   "source": [
    "subdf                       = pd.read_csv('/content/SampleSubmission.csv')\n",
    "subdf['income_above_limit'] = model.predict(test_df)\n",
    "subdf.to_csv('Sample_submission_1.csv',index=False)\n",
    "subdf['income_above_limit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKBJLRQoqZza"
   },
   "source": [
    "How to get better scores:\n",
    "1. Feature engineering is the key. Refer to the variable dictionary and create meaningful features which can boost the score\n",
    "2. Try out different models and categorical data preprocessing (read about categorical encoding) because a lot of features are categorical. \n",
    "3. Feature selection with feature importance \n",
    "4. Keep a check on classification report to observe overfitting and underfitting and select appropriate hyper-parameters to tune.\n",
    "5. Suitable probability threshold selection as shown above. \n",
    "6. Be creative while selecting validation split \n",
    "For example:- Use Stratified K folds, grouped K folds, repeated stratified k folds, train test split with stratification etc \n",
    "7. Ensemble multiple models to get a stable prediction. \n",
    "8. Be creative and may the best model win üèÜ üèÜ üèÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XF8r3ySkA5iP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
